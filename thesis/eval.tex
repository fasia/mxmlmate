\section{Evaluation}
\label{sec:evaluation}
From the previous sections you should be familiar with the main ideas and even some technical details behind
the approach that is the main focus of this document. However, every approach is only worth as much as it can
offer. To determine this for the presented implementation - \xmlmate, an empirical evaluation on several test
subjects has been carried out, and the results are documented in this section.
\subsection{Evaluation Setup}

Due to the random nature of the genetic algorithm that is at the heart of \xmlmate, it is necessary to perform
multiple runs of each experiment in order to properly ascertain its effectiveness and efficiency statistically
by considering the average values. At this point it is of particular importance to define what efficiency and
effectiveness mean in regards to this evaluation. 

Let \emph{effectiveness} be the measure of whether or not defects have been found in the time frame allocated
for an experiment run. Similarly, let \emph{efficiency} be the defect detection count for the same time frame.

One \emph{experiment} consists of running \xmlmate for one hour with a limit of 2200 elements per \xml instance
as well as a maximum recursion depth of 12 for optional elements, meaning that no optional elements are generated
for subtrees of depth 13 or more. Additionally, the maximum number of \xml files in the population being
evolved is limited to 500, which is kept consistent across the normal and the singleton population (see
\cref{sec:memcov}) modes as follows: in the singleton population usage scenario, where single files are the
main focus of the evolution process, the limit is expressed simply as a single suite with a maximum of 500
individual files in it, while in the other use case, where the result of an evolution is a suite of files, the
limit is enforced as a population of 25 suites with a maximum of 20 files in each.

To produce statistically significant average result values, for each fitness function and for each test
subject an experiment was run 10 times. This setup was replicated for the two modes of testing: with and
without the mechanism for producing schema-invalid values.

The experiments were carried out on a virtual machine equipped with 8 CPU cores running at 2.60GHz and 64 Gb
main memory, which, however, was far more than needed because the \java part of \xmlmate{} (the process taking
up the most memory) only needed 2Gb for in-memory \xml instance handling. The deployment configuration
of the \xmlmate system consisted of the following components:

\begin{itemize*}
  \item 1 instance of the \java \xmlmate core component
  \item 1 load balancer between the core component and converter instances
  \item 8 format converter instances
  \item 1 load balancer between the converters and the workers
  \item 8 worker instances (worker = test driver + test subject instance + pintool)
  \item 8 lifeguards for their respective workers
\end{itemize*}

The setup can be imagined as in \cref{fig:components} with $n = m = 8$.
It must be noted that no converters were used for testing \libxml as it consumes inputs as generated
by the \xmlmate core component directly, therefore only one load balancer was needed between the generator
instance and the worker instances.

\subsection{Test Subjects}
While \cref{sec:formats} has introduced you to the file formats used in this document, the following sections
will give you a short description of programs that were used during the evaluation, which work with those
formats. Each test subject was chosen as the leading library for processing its corresponding file format.
\tocless\subsubsection{libxml2}
\libxml\footnote{\url{http://www.xmlsoft.org/}} is a library for processing generic \xml documents,
which is written in {\small C}. It is widely used and provides bindings to many programming languages
including \cpp, {\small C\#}, \python{}, {\small Ruby}, {\small PHP5}, and {\small Perl}. Furthermore, it was
designed for utmost portability, such that it works on a wide variety of operating systems such as
Linux, Windows, CygWin, MacOS, RISC Os, OS/2, and others. Notably, the Google Chrome
browser uses \libxml internally, which further shows the library's popularity.

The library is extremely well tested, and in its current version 2.9.2 it passes all tests from the very large
OASIS XML Test Suite\footnote{\url{https://www.oasis-open.org/committees/xml-conformance/xml-test-suite.shtml}}
consisting of more than 1800 individual tests. It is, therefore, very unlikely for \xmlmate to find any
vulnerabilities or even defects, especially because it specializes in generating \emph{valid} XML files, and
even more so in this case, where only \texttt{xhtml} files, which do not represent the entirety of the \xml
specification, were used for testing.

Since in its main functionality this library acts like a parser, the interface for performing testing with
\xmlmate is correspondingly simple: the test driver engages the library under test by passing it a file to be
parsed and loaded into memory, whereafter the file is unloaded. No additional walks are performed, which
sacrifices a lot in terms of achieved fitness scores, but wins out multiple times over in the
overall execution speed.

% download from https://git.gnome.org/browse/libxml2/refs/tags

\tocless\subsubsection{libpcap}
\libpcap\footnote{\url{http://www.tcpdump.org/}} is a library for working with the \pcap file format
maintained by the \emph{Tcpdump Group}. It is used in the popular tools tcpdump, ngrep, Wireshark, Snort and
nmap, among others. The main objective of \libpcap is to provide its users with a
platform-independent API for network packet capturing. The API is designed to work with the {\small C} and \cpp
programming languages, but there exists a number of bindings to other languages as well such as \python, \java,
{\small C\#}, or {\small Ruby}.

While it first and foremost provides a nice abstraction layer above the operating system-specific live packet
capturing mechanism (which every operating system vendor seems to have designed differently), \libpcap
also provides an interface for manipulating packets already captured and available in the \pcap file format.
This is the interface through which testing with \xmlmate is performed as it seems otherwise rather
infeasible to simulate a network card interface in order to engage \libpng on the live capturing
interface. One consequence of choosing the offline file reading interface is, however, that only a relatively
small portion of the program code of \libpcap can be possibly engaged. This limitation has to be kept
in mind when assessing the results of the empirical evaluation presented in this section. It is also noteworthy
to mention that to date there are no known vulnerabilities in \libpcap itself, while both tcpdump and
Wireshark seem to have corresponding CVE entries in such Internet vulnerability databases like
\url{http://www.cvedetails.com}.

\tocless\subsubsection{libpng}
\libpng\footnote{\url{http://www.libpng.org/}} is a ubiquitous library for manipulating, reading,
writing and generally handling \png files. It is available freely under a permissive license, and thus very
widely used in a wast number of both free and proprietary software products. The \libpng library is
written in the {\small C} programming language with a dependency on the \texttt{zlib} library, on which it
relies for data compression. It is considered to be the reference implementation for all possible interactions
with \png files.

In its 18 years of existence, the library was extensively tested as it evolved and matured. Ever since the very
early versions it provides \emph{pngtest} - a utility that allows to test a \png file for conformance with the
\png file format. This process involves reading and parsing the file, storing it in memory, performing several
transformations on it, writing it out to disk again, and comparing the result with the original input file.
This seems like a good representation of the usual use case for this library, which is why it was chosen as
the interface at which testing with \xmlmate was performed. The test driver for \libpng basically
consists of pngtest itself slightly modified to enable communication with \xmlmate via \zmq. In order not
to waste computational resources, one limitation was imposed on the images generated by \xmlmate: their size
was limited to $2\times 2$ pixels because most defects found in \libpng originate from specific values
in the metadata rather than image data itself.

%\subsubsection{libflac}
\subsection{Results}
In the following sections the results of the conducted experiments are presented: first a general overview is
given of the fitness scores achieved by each fitness function alongside explanations and observations,
followed by examples of vulnerabilities found in the \libpng library, and finally, additional defects found in
related programs are discussed.

\subsubsection{Fitness Scores}
The first experiment serves to investigate the practicality of each subject-oriented fitness function
presented in \cref{sec:fit} when applied to the proposed test subjects. To accomplish this, a comparison
between the fitness scores achieved by each function for each subject is needed.
% One exception is the Schema Coverage fitness function, which is ignored here because using it only depends on
% the format specification and its quality and is thus independent from test subjects.
The subjects used for this comparison were taken from their respective official repositories, the versions
used were as follows: \libpng \texttt{1.5.4}, \libpcap \texttt{1.3.0}, and \libxml \texttt{2.8.0}. While the
\libpcap and \libxml versions were chosen as the most easily obtainable on the current stable branch of the
Debian operating system, the version of \libpng was chosen such that both fitness values for a general
comparison and defect search could be carried out at least in part simultaneously and with minimum effort.
(This particular version of \libpng has an unchecked division by zero defect.)

\Cref{tbl:fitness} shows an overview of
the fitness scores achieved by each fitness function. These scores are average values computed over the ten
experiment runs that were carried out for each fitness function. For each test subject the table has two rows
for both of the input generation modes: the one that strictly adheres to the format specification and thus
produces schema valid inputs, and the one that is able to ignore some specification rules. The former mode is
marked with a ``+'' sign in the \emph{Schema Valid} column, while the latter one is marked with a ``-'' sign.

\begin{table}[H]
\small
\centering
\begin{tabular}{|r|c|r|r|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{Subject}  & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Schema\\ Valid\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}BBL\\ Coverage\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}BBL\\ Succession\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Memory\\ Access\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}$\downarrow$ Division\\ by 0\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Integer\\ Overflow\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Buffer\\ Overflow\end{tabular}} \\ \hline \hline
                               & +                                                                           &              143.2                                                          &                          185.3                                                &                                470                                           &                          no divs                                           &                          7.0369E13                                             &                          2175400                                                               \\ \cline{2-8} 
\multirow{-2}{*}{libpcap}      & \cellcolor[HTML]{C0C0C0}-                                                   & \cellcolor[HTML]{C0C0C0} 152.8                                              & \cellcolor[HTML]{C0C0C0} 192                                                  & \cellcolor[HTML]{C0C0C0} 470                                                 & \cellcolor[HTML]{C0C0C0} no divs                                           & \cellcolor[HTML]{C0C0C0} 7.0369E13                                             & \cellcolor[HTML]{C0C0C0} 2175408                                                               \\ \hline \hline
                               & +                                                                           &                          14963                                              &                          11507                                                &                          4007                                                &                     1                                                      &                          4.6115E18                                             &                          2102329                                                               \\ \cline{2-8} 
\multirow{-2}{*}{libpng}       & \cellcolor[HTML]{C0C0C0}-                                                   & \cellcolor[HTML]{C0C0C0} 15330                                              & \cellcolor[HTML]{C0C0C0} 11657                                                & \cellcolor[HTML]{C0C0C0}  66493                                              & \cellcolor[HTML]{C0C0C0}      1                                            & \cellcolor[HTML]{C0C0C0}              4.6115E18                                & \cellcolor[HTML]{C0C0C0} 2102329                                                               \\ \hline \hline
                               & +                                                                           &       18650                                                                 &                          12576                                                &                            858058                                            &                                      128                                   &                          4.4175E18                                             &                          73728                                                                 \\ \cline{2-8} 
\multirow{-2}{*}{libxml2}      & \cellcolor[HTML]{C0C0C0}-                                                   & \cellcolor[HTML]{C0C0C0}    18987                                           & \cellcolor[HTML]{C0C0C0} 12791                                                & \cellcolor[HTML]{C0C0C0}   848560                                            & \cellcolor[HTML]{C0C0C0}    104.4                                          & \cellcolor[HTML]{C0C0C0} 4.6216E18                                             & \cellcolor[HTML]{C0C0C0} 73728                                                                 \\ \hline
\end{tabular}
\caption{Average Fitness Scores}
\label{tbl:fitness}
\end{table}

\paragraph{General Remarks} ~\\
The fitness scores differ significantly across the different fitness functions even for the same subject,
which is understandable because each of the functions has different metrics, which makes their scores
largely incomparable because the functions are designed to measure different characteristics of a
program's execution.

What might seem somewhat surprising however, is the great variance in scores for the same fitness function
across multiple subjects. For example take the difference for the Basic Block Coverage fitness function - it
has a score of 15330 for \libpng and only 152.8 for \libpcap. 
A similar difference can also be seen in all fitness functions whose scores are directly related to the
complexity or richness of the inputs i.e.\ also the Basic Block Succession and the Memory Access fitness
functions. Two main reasons can be given to explain this difference:

The first is that even though both libraries provide extensive parsing and data manipulation capabilities,
the formats they are designed to process differ significantly in complexity themselves. This causes a
corresponding difference in parts of the code responsible for checking inputs for consistency and keeping
the data that way during manipulation operations.

The second reason is in the nature of the test driver used with each subject: while the test drivers for
\libpcap and \libxml mostly exercise parsing capabilities, the test driver used for \libpng additionally
engages several data consistency checks and transformations as well as writing of output.

To provide further hints on possible idiosyncrasies found in \cref{tbl:fitness} please note that the four
rightmost fitness functions use the singleton population evolution model, which offers performance benefits,
which, in turn, might have an effect on the utilization of the time frame allotted for an experiment run and
thus the resulting fitness scores.

\paragraph{Basic Block Coverage Fitness Function} ~\\
The Basic Block Coverage fitness function provides a very basic and rather crude method to reach into a
program's logic. Possibly somewhat counterintuitively, it does not provide a way to estimate the complexity of
the application under test on its own. This is mainly because there is no guarantee that the entirety of
the program's functionality can be reached at all from the entry point chosen in the test driver. So this fitness function
rather provides a measure of the quality of the test harness itself, which can serve as a tool for improving
its quality, and also for interpreting results achieved by other fitness functions more accurately by adjusting
expectations accordingly.

This does not mean, however, that this fitness function does not provide any indication as to the complexity of
the program under test at all. Its scores are indeed proportionally indicative of the number of statements
present in the part of the code reachable from the test driver's entry point. Using this interpretation it can
be concluded that the extent of functionality engaged in \libpcap was an order of magnitude smaller than that
of \libpng and \libxml.

\paragraph{Basic Block Succession Fitness Function} ~\\
As a more involved fitness function also concerning itself with basic blocks, the Basic Block Succession
fitness function provides another view on the complexity of the program behavior engaged by the test driver.
While the previous fitness function merely indicates the extent of a subject's engaged functionality, this
fitness function illuminates its interconnectedness. It shows not only how much functionality has been reached,
but also how it was reached i.e.\ which paths were taken during execution.

The findings of this fitness function seem to suggest that while the complexity gap between \libpcap and the
two other subjects found by the previous fitness function is legitimate, the gap of approx.\ 3500 BBLs between
\libpng and \libxml is an overestimation as the difference in interconnectedness is not appropriately
large. Because the test drivers are not equivalent and the \libpng driver involves more complex
behavior, the findings of this fitness function lead to the conclusion that the large number of execution
subpaths taken by \libxml must result from the increased expense of parsing its text based input format, which
\libpng is lacking due to its inputs being in binary format already. This possibly also explains the larger
number of BBLs an input has to pass through in \libxml.

\paragraph{Memory Access Fitness Function} ~\\
The scores achieved by the Memory Access fitness function indicate how much memory is accessed by the
application under test when processing an input file. Please note that this function is somewhat simplistic.
Its values are not always proportional to the size of the inputs as there is no guarantee that the program
under test will load all data into its memory at once - it could just as easily process the input using a
buffer or a sliding window technique only ever reusing the same constant memory space.
Additionally, the scores of this fitness function are heavily influenced by how many operations on the input
data are performed by the tested application - e.g.\ the \libpng driver first reads the data, manipulates
it, and finally writes it out again, performing at least three passes over not necessarily the same regions of
memory. Nevertheless, the Memory Access fitness function is a very helpful tool in detecting program defects
related to memory operations, as you will see in the next section. 

Taking a closer look at the outcome of the experiments, the result for \libpng is perhaps in need of special
attention because of the very large difference between scores for schema-valid and invalid inputs of 4007 and
66493, respectively. To explain this, please note that \libpng loads its input entirely into memory, which
makes the values for the Memory Access fitness function proportional to the input size in this case. Now, with
the generator occasionally violating schema restrictions based on numbers, the $2\times 2$ pixel limitation is
happily ignored and images with very large dimensions are produced, which easily consume significantly more
memory. E.g.\ if the generator produces image data of size $12 \times 10$, the memory consumption is increased
30-fold. This increase leads to a corresponding increase in the fitness score, which favors the production
of even larger images generation after generation.

\paragraph{Division by Zero Fitness Function} ~\\
In accordance with their respective descriptions (listed in \cref{sec:fit}), all fitness functions
presented in \cref{tbl:fitness} are maximization functions with the exception of the Division by Zero fitness
function, which is a minimization function - it is therefore marked with a downward arrow ($\downarrow$) in the
table to signify that smaller values are considered better.

This particular fitness function was unfortunately useless in the case of \libpcap because no division
instructions could be detected. There might still be division instructions, which were simply undiscovered due
to the limited reach of the test driver, or the division instructions used were of a kind unsupported in the
current version of the fitness function such as SIMD instructions that deal with entire arrays of data
simultaneously.

The lowest average divisor value reached for \libxml was 128, which is already better, but still nowhere near a
desired division by zero. The minimal observed divisor value was 10, but it only appeared twice over the
course of the ten experiments. It seems that this library does not have a distinctive correlation between user
data and the divisions performed.

It might seem strange that the best score for the Division by Zero fitness function for \libpng is 1 even
though the version of \libpng used for this particular experiment has an unchecked arithmetic exception
defect. Clearly a value of 0 would be expected, especially considering that this defect is, in
fact, found by \xmlmate rather easily. The explanation for this score is actually quite simple: the inputs
that would have a perfect score of 0 are actually crashing the tested program, which means that their
fitness is only reported as ``crashed'' to the genetic algorithm. It is, of course, possible to assign
the best fitness score to such inputs, however this would only lead to more crashes due to the same
defect, and also severely hinder the diversity of the gene pool as more and more inputs would simply become
very similar because of the perfect fitness score. Therefore such inputs are instead given the worst possible
fitness score in order to be phased out of the population to make room for new mutations. This way, at the end
of the evolution only individuals that are as close as possible to having a perfect score without actually
reaching it are reported as the best individuals produced during the process. This, of course, is only an
implementation detail, and a value of zero could be justifiably put in \cref{tbl:fitness}.

% TODO \paragraph{Integer Overflow Fitness Function} ~\\
% TODO \paragraph{Buffer Overflow Fitness Function} ~\\

% TODO \paragraph{Impact of Schema Violation} ~\\
% explanation for the one under it => xml is xml, no validation, local search eats time


% \begin{table}[H]
% \small
% \centering
% \begin{tabular}{|c||r|r|r|r|r|r|}
% \hline
% \begin{tabular}[c]{@{}c@{}}Schema\\ Valid\end{tabular} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}BBL\\ Coverage\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}BBL\\ Succession\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Memory\\ Access\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Division\\ by 0\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Integer\\ Overflow\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Buffer\\ Overflow\end{tabular}} \\ \hline
% + & ? & ? & ? & 13.2 & 9.2 & 12.9 \\ \hline
% - & 1.8 & 1.6 & 1.2 & 12.0 & ? & 16.1 \\ \hline
% \end{tabular}
% \caption{Average Division by Zero Defect Count for \texttt{libpng 1.5.4}}
% \label{tbl:png:crashes:avg}
% \end{table}
% Explanation for div0^{ls} < div0: ls uses the time resource, but doesn't provide a benefit because the zero is already legal


\subsubsection{Detected Vulnerabilities}
\Cref{tbl:png:vulns} shows a selection of known vulnerabilities in \libpng chosen as indicators for determining
the effectiveness and efficiency of \xmlmate as a search-based tool for defect detection. The vulnerabilities are
listed alongside the corresponding vulnerable program version and their CVSS - a score on a scale from 0 to 10
that indicates the severity of a vulnerability.
% TODO mention that these were selected as crash inducing because that's the class we can detect

% as found in https://wiki.mmci.uni-saarland.de/syssec/Vulnerabilities/libpng
\begin{table}[H]
\centering
\begin{tabular}{|r|c|r@{}l|l|}
\hline
Vulnerability & CVSS  & \multicolumn{2}{c|}{Version} & Short Description \\ \hline \hline 
CVE-2011-3328 & 2.6   & 1&.5.4 	& Arithmetic exception: division by zero   \\ \hline  % div0(10/132)
CVE-2011-2691 & 5.0   & 1&.5.2	& Null pointer dereference in error message \\ \hline % buf(36/5k), mem(10/430)
CVE-2013-6954 & 5.0   & 1&.6.6  & Null pointer dereference on empty palette \\ \hline % buf(10/157), bbl2, mem
CVE-2011-3048 & 6.8   & 1&.5.9  & Unchecked failing malloc in constrained memory \\ \hline % mem(127/3449), buf 
% TODO write about restricting memory to 450 MB
CVE-2008-1382 & 7.5   & 1&.2.25 & Segmentation fault on unknown chunks of size 0\\ \hline % bbl2(20/0), schemaCov
\end{tabular}
\caption{Vulnerabilities in \libpng Versions}
\label{tbl:png:vulns}
\end{table}

\Cref{tbl:png:rates} shows the average detection rate for each of the vulnerabilities from \cref{tbl:png:vulns}
as well as the fitness function used in each experiment.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|r|}
\hline
Vulnerability                  & Fitness Function & \#found / hour \\ \hline \hline
CVE-2011-3328                  & Division by Zero & 13.2           \\ \hline
\multirow{2}{*}{CVE-2011-2691} & Buffer Overflow  & 138.8          \\ \cline{2-3} 
                               & Memory Access    & 43             \\ \hline
CVE-2013-6954                  & Buffer Overflow  & 15.7           \\ \hline
\multirow{2}{*}{CVE-2011-3048} & Memory Access    & 27.2           \\ \cline{2-3}
							   & Integer Overflow & 6.3			   \\ \hline
CVE-2008-1382                  & BBL Succession   & 623            \\ \hline
\end{tabular}
\caption{Vulnerability Detection Rates}
\label{tbl:png:rates}
\end{table}

% TODO update for more FFs and CVE-2008-1382
% TODO Adde short descriptions of vulnerabilities
Overall it is safe to say that \xmlmate is both effective and efficient as a vulnerability detection tool.
% mention no false positives

% XXX add a 24h run on the newest version of libpng as an additional bonus experiment 
% ??? mention problem of failure masking -> one crash makes another undetectable

\subsubsection{Additional Defects}
In the process of running the experiments needed for this evaluation several defects were found in programs
related to the used file formats, which were not specifically targeted. Here are the most interesting of them.
\paragraph{Gimp} ~\\
The GNU Image Manipulation Program\footnote{\url{http://www.gimp.org/}} (\texttt{gimp}) is an open source
application, which is capable of opening and editing \png files. The latest available version of \texttt{gimp}
is \texttt{2.8.2} on the current stable branch of the Debian operating system, which, at the time of writing,
is ``Wheezy''. In this version \texttt{gimp} has a defect that results in immediate program termination upon
opening a specially crafted \png file due to a segmentation fault. To cause this, the \png file must
simply contain a \texttt{PLTE} chunk with an empty data region. This defect is no longer present in later
versions of \texttt{gimp}, although it is not listed at \url{http://www.cvedetails.com}.
\paragraph{Wireshark} ~\\
Wireshark\footnote{\url{https://www.wireshark.org/}} is a cross-platform open source program for network packet
inspection and analysis. Its latest version available on the aforementioned Debian configuration is
\texttt{1.8.2} and this version has the following defect, which is also present in the same version on the
Windows 7 x64 operating system: when opening a specially formed \pcap file the application terminates and
prints the following error message on the console:

{\small ("Null pointer passed to bytes\_to\_hexstr\_punct()", group=1, code=4) in file to\_str-unt.h}

This defect is not present in the newest version of Wireshark, but it is also not listed among the known
vulnerabilities at \url{http://www.cvedetails.com}. This particular defect was found when \xmlmate reported to
have crashed a \libpcap worker instance. Unfortunately, the crash in \libpcap itself could not be reproduced,
but while trying to inspect the generated \pcap file, Wireshark crashed instead. It is still possible that
this file actually crashed \libpcap, but some other conditions were involved not directly controlled by the
experiment; this investigation could present a topic for a future work item.
\paragraph{Firefox} ~\\
Input files for testing \libxml are generated in the \texttt{xhtml} format, which is primarily a web page
description format, so several of them were opened in different web browsers for manual inspection. As a side
effect several files exhibiting interesting behavior in modern browsers were found. The most notable is a 161KB
sized file that causes the latest Firefox web browser (version \texttt{37.0.2} at the time of writing) to
consume several gigabytes of memory and promptly crash. The size of the input file can most certainly be
reduced even further by using a variation on the delta debugging technique\cite{zeller2002simplifying}. 

% ??? mention chrome warning
% Yet another relatively small \texttt{xhtml} file of 549KB causes the latest Google Chrome web browser (version
% \texttt{42.0.2311.90} at the time of writing) to freeze up for a considerable amount of time and then display a
% notification asking the user whether to proceed with the efforts of loading the page. If allowed to proceed,
% the page is rendered after another considerable stretch of time.


\subsection{Threats to Validity}
As any empirical study, this evaluation is subject to several threats to validity.
First, as regards \emph{external validity}, the subjects used in this study may be far from representative of
the entirety of all applications now compatible with \xmlmate, and may even be uncharacteristic among their
respective application class.

Concerning \emph{internal validity} it must be kept in mind that search-based testing is random in nature, and
thus the results may vary greatly over multiple experiments. Some effort was made to mitigate this problem by
performing multiple runs and taking the average results. However, it is unclear if a much larger sample size
would alter the results in any significant way.

Furthermore, the choice of many of the parameters for the genetic algorithm such as population size,
genetic operation probabilities, or chromosome limitations might not have been optimal for the chosen test
subjects.

Regarding \emph{construct validity} it is uncertain if the metrics used in the evaluation are well suited
to adequately measure the usefulness of the presented approach.
 
% partial bias -> pcap schema and converter was written by me, I tried to adhere to the specs