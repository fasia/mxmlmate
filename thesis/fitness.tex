%### 
\subsection{Fitness Functions}
%### 
One requirement of this work was to provide \xmlmate with the concept of pluggable fitness functions and 
give several examples that demonstrate the versatility of the approach. Luckily, \evosuite already provides 
a \texttt{FitnessFunction<T extends Chromosome>} generic interface, which can be made use of on multiple 
levels of abstraction. To provide the pluggability on the level of  entire test suites there are implementers of
\texttt{FitnessFunction<XMLTestSuiteChromosome>} and on the level of single test inputs ones of 
\texttt{FitnessFunction<XMLTestChromosome>}. 
Secondary evolution objectives have a very similar type topography; secondary objectives are sometimes useful 
as ``tie breakers'' when a decision between two chromosomes has to be made when both have equal fitness values,
but possess subtle differences that e.g. might lead to slightly more or less favorable crossover behavior in 
the future. The next sections describe some of the new fitness function implementations available with the 
extended and improved \xmlmate.
%### 
\subsubsection{Schema Coverage Fitness Function}
%###
\label{sec:fit:schema}
The very first extension from pure \java branch coverage and exception oriented fitness functions employed 
previously in \xmlmate is the \texttt{Schema Coverage} fitness function. Its goal is to maximize the number 
of schema rules used in the generated inputs. The reasoning behind this is that inputs which exhibit most of 
the features described in the specification should also trigger more functionality in the program under test. 
More concretely the schema coverage fitness value is computed as the average of the folowing three values:
\begin{description}
  \item[element coverage] describes how many element declarations were instantiated in the generated inputs, 
  including substitution group members. A substitution group consists of jointly declared elements, which 
  can take each other's place in the \xml. Additionally, the element coverage can reflect how many branches 
  of choice particles as well as how many of the optional elements are present in the produced files.
  \item[attribute coverage] is, similarly, a measure of how many attribute declarations were utilized, so its 
  value depends on both the optionality of the declarations and the number of generated elements (or more 
  precisely the number of elements carrying the corresponding attribute declarations).
  \item[transition coverage] probably sounds a little out of place, so let me first explain how the concrete 
  values for elements and attributes are generated. One of the most implementation-wise challenging features 
  of the \xsd{} {\small Language} is the \texttt{pattern} facet that describes valid values of an element or 
  attribute by means of a regular expression (that has a different grammar than that of \java, \python or 
  {\small Perl} regular expression languages, which, as an aside, presented another whole technical challenge). 
  In order to be able to provide values conforming to those regular expressions an automaton based representation 
  has been put in place for the types of the elements and attributes. In fact, it worked so well, that there 
  was longer any need in representing any type in a way other than by means of a deterministic finite
  automaton. This means that all types, including \texttt{string}, \texttt{int}, \texttt{short}, \texttt{hexBinary},
  have a corresponding automaton, which is cached in memory for performance reasons. Furthermore, with this representation 
  it is very easy to apply additional facets like restrictions on length or number of fractional digits just by 
  performing automaton based operations like union and intersection. 
  Coming back to the topic of schema coverage, the transitions in these automata are the subject of its 
  transition coverage component which describes how many of the transitions of all the automata are being 
  used in the generated \xml files. This measure is indicative of the overall proportion of covered value space 
  over all type definitions regarded together.
\end{description}

If the element, attribute and transition coverages are designated as $c_e$, $c_a$, and $c_t$ respectively and
are numbers between zero and one, the overall schema coverage fitness value can be computed as $1 -
\frac{c_e+c_a+c_t}{3}$, which makes this a \emph{minimization} fitness function, meaning that the genetic
algorithm of \evosuite will try to get the value as close to zero as possible, which is its default behavior.
    
Contrary to the initial expectations, \improvement{mention in the Evaluation section} the results do not 
necessarily show a strong correlation between high schema coverage and some kinds of code coverage like 
branch or basic block coverage. This can be explained by the fact that it is not the abundance of different 
input values, but rather some specific values themselves, that are responsible for penetrating deeper into a 
program's logic and thus increasing the code coverage.

However, the schema coverage is a good evolution guide for the purpose of creating a diverse starting population
which can then be further evolved with different purposes - be it code coverage of particular program regions, or
other kinds of fitness criteria like closeness to integer overflows and the like.

Furthermore, there is no need in powering up any of the infrastructure involved with usual ways of measuring 
fitness - no system under test, no brokers or converters are needed to compute the schema coverage fitness. 
There is also no need for \texttt{IO} operations, as all \xml tree evaluations are processed in-memory.

%### 
\subsubsection{Basic Block Coverage Fitness Function}
%###
A more or less direct transfer of \evosuite's \texttt{Instruction Count} fitness function is the 
\texttt{Basic Block Coverage} fitness function, which aims to maximize the number of basic blocks 
executed in the program under test. This fitness function makes heavy use of the {\small Intel} \pin
instrumentation framework - in particular its definition of a basic block: a single entrance, 
single exit sequence of instructions in the program under test. However, because \pin is discovering the 
program dynamically as it is executed, its view of basic blocks can be somewhat unconventional. As an 
example consider the code on the left in \cref{lst:bblcode}, which, when compiled for the IA-32 
architecture, will yield instructions approximate to the ones on the 
right\footnote{From \url{https://software.intel.com/sites/landingpage/pintool/docs/67254/Pin/html/\#GRAN}}.
Classically speaking, each \mintinline{nasm}{addl} instruction is its own basic block; however, over the course
of program execution, when the different switch cases are entered, \pin will generate basic blocks, which
contain four instructions as the \texttt{.L7} case is entered, three basic blocks as the \texttt{.L6}
case is entered, and so forth. Furthermore, \pin breaks basic blocks on some other instructions like 
\texttt{cpuid}, \texttt{popf}, and \texttt{REP} prefixed instructions. This leads to a slight divergence 
from the expected values, but for the purposes of representing code coverage this has no negative 
consequences. As a matter of fact, this phenomenon is actually somewhat reminiscent of the LCSAJ
metric\cite{Hennell:1976:PA}, which only increases its value as a fitness score component.
\begin{listing}[h]
\centering
\begin{minipage}[b]{0.49\textwidth}
	\centering
	\begin{ccode*}{linenos=false,frame=bottomline}
	switch(i) {
        case 4: total++;
        case 3: total++;
        case 2: total++;
        case 1: total++;
        case 0:
        default: break;
    	}
\end{ccode*}
	Example C Code
 \end{minipage}
%
 \begin{minipage}[b]{0.49\textwidth}
  \centering
  \begin{minted}[frame=bottomline]{nasm}
.L7:
        addl    $1, -4(%ebp)
.L6:
        addl    $1, -4(%ebp)
.L5:
        addl    $1, -4(%ebp)
.L4:
        addl    $1, -4(%ebp)
\end{minted}
  IA-32 Instructions
 \end{minipage}
 \caption{Example for Basic Block Idiosyncrasies in \pin}
 \label{lst:bblcode}
\end{listing}

The \texttt{Basic Block Coverage} fitness function is the first, but not the only to use the new binary 
backend feature set. Therefore it profits from an abstract fitness function prototype designed specifically 
to work with binary test subjects, which abstracts away and manages the responsibilities of communicating 
with the backend system, be it potential format converters, or the targeted program controlled by test 
drivers and monitored by pintools.
Like all subclasses of this \texttt{BinaryBackendFitnessFunction} the \texttt{Basic Block Coverage} 
fitness function consists of two components: a \java class and a pintool written in \cpp.

The pintool's task is to instrument the targeted binary in such a way, that whenever it executes a 
basic block, which belongs to the image of interest (e.g. \texttt{libpng} - a parameter given at the start),
its address is recorded in a set data structure. When the test driver, which is responsible for running 
the program under test, signals the completion of the processing of the currently evaluated inputs, 
the pintool sends the set as a packet to the \java class in \xmlmate.

It is the purpose of the \java class to interpret the data received from the pintool.
In the case of the \texttt{Basic Block Coverage} fitness function, the data received represents the
set of starting addresses of the basic blocks that were found to be executed in the program under test. 
For each \xml in a test suite an individual set must be maintained in order to be able to profit 
from result caching: when a test suite is modified during the evolution step, not necessarily all
of its tests must have been changed and thus not all tests must be passed to a test driver for reevaluation, 
their previous result can be reused because it is still up-to-date. Virtue of the fact that the pintool 
already builds a set out of the addresses of the executed blocks on its side, the result for a single 
\xml can be stored as an array (\texttt{long[]}) without having to wrap a \texttt{Set} container around it.

However, when it comes to calculating the number of unique basic blocks covered by an 
entire test suite, each time a new set must be created which consists of the union of 
the individual arrays belonging to all \xml files in the suite. At this point the very efficient
\texttt{TLongHashSet} implementation from the \emph{GNU Trove} collections mentioned in \cref{sec:trove} 
is very useful - especially so because it stores the \java primitive values directly instead of \emph{boxing}
them in wrapper objects like the standard \java collections do. This way Trove saves space and is faster.

The actual fitness value of a test suite according to the \texttt{Basic Block Coverage} fitness function 
is the number of unique basic blocks executed by all \xml files in the suite. Due to the dynamic nature 
of instrumentation with \pin it is impossible to know the total number of basic blocks in the program 
under test. One consequence of this is that this fitness function must be a maximization function - meaning
that higher fitness values are considered better, which is somewhat atypical for \evosuite. Even though it has
some degree of support for maximization functions, multiple code defects surfaced during the development that
mostly had to do with hard-coded assumptions about the fitness function always being a minimization function.

%### 
\subsubsection{Basic Block Succession Fitness Function}
%### 
The \texttt{Basic Block Succession} fitness function is an extension of the \texttt{Basic Block Coverage}
fitness function in that it is also heavily based on observing the execution of basic blocks. However, this
extension also considers the order in which the blocks are executed. More specifically, for each basic block
this fitness function aims to maximize the number of unique basic blocks that are executed immediately after.
Put differently, the \texttt{Basic Block Succession} fitness function provides guidance for maximizing the
number of control paths of length two taken by the program under test. According to the
author\footnote{http://lcamtuf.blogspot.de/2014/08/a-bit-more-about-american-fuzzy-lop.html} of the
\texttt{American Fuzzy Lop} smart fuzzing tool\cite{afl} this strategy strikes a good balance between
fitness function complexity and effectiveness, and is very helpful in finding defects.

To accomplish its task, the pintool maintains a map, which contains an address for each basic block that was
observed to be executed as a key. Each key is assigned as value a set of all basic blocks that were executed
immediately after it - be it by fall-through, call, return, or conditional jump. 

The \java part of the \texttt{Basic Block Succession} fitness function receives this map data for each file in
the suite, and collates and merges it into a single basic block successor graph in order to determine the
fitness of the entire suite. The fitness value is then calculated as the number of edges of this graph. 
\improvement[inline]{Add graph example (+merging)}

%### 
\subsubsection{Memory Access}
%###
\label{sec:memcov}
All previously described fitness functions are aimed at increasing a coverage of one sort or another; however,
in the context of using \xmlmate to try and find vulnerabilities in the system under test, this is not
necessarily the most promising approach. In order to be able to find vulnerabilities having to do with
operations such as reading from and writing to memory, such as null pointer dereferences or buffer underflows
or overflows, the first na\"\i{}ve approach is to aim for a maximum number of memory reads and writes at a
maximum of addresses. This is the idea behind the \texttt{Memory Access} fitness function, which like its
cousin \texttt{Basic Block Coverage} fitness function, is a maximization function, with which it also shares
the \texttt{BinaryBackendFitnessFunction} as an ancestor. Once again, this fitness function consists of a
pintool and its complementary \java class. 

The pintool stores memory addresses that were accessed by the program under test in a set, which it sends to
the \java class upon test completion.
The \java class is very similar to that of its cousin fitness function, in fact they share the entirety of
their code.

However, upon consideration of the current use case, the distinction between test suites and tests as
prescribed by \evosuite no longer seems appropriate - what we actually want instead is to evolve a population
of single \xml files in order to find one individual file that triggers faulty behavior. To satisfy this newly
appeared need, the concept of a \emph{singleton population} was introduced.

Using a singleton population is a compromise between \evosuite's need for the presence of test suites to
evolve, and our need for only one set of files to be evolved. When the singleton population is used, the
population size of the genetic algorithm is limited to only one suite and the genetic algorithm itself is
modified accordingly, amongst other things by disabling elitism on the test suite level. Elitism is a
mechanism which ensures that the best individual (the best test suite in \evosuite's case) survives into the
next generation unaltered, so that fitness may never decrease from one generation to the next. Note,
that using elitism does not prevent the best individual from reproducing - its copies may do so freely, and
usually this is what leads to an improvement in fitness. 

With the singleton population in place, there is only one test suite to evolve, which means that the fitness
must now be calculated differently. The fitness of this single suite can no longer reflect the cumulative
result over all the tests, but must rather correspond to the fittest one of them, which, at the end of the
evolution, will be considered the result of the search, or the pinnacle of evolution, if you will.

This routine can be somewhat improved by adding a secondary evolution objective, which considers a
mutation of the test suite as an improvement even if it has the same number of memory accesses in the best
test, yet cumulatively its tests cause more unique memory addresses to be accessed. 

As a technical aside, I want to mention that there were considerations to model the use case of evolving a
set of files as a population of singleton test suites each containing a single file instead of the currently
employed population consisting of one suite. This, however, would have been unfavorable to the parallelization
efforts as per \cref{sec:par} because \evosuite does not allow parallel evaluation of test suites without some
major intrusion in its code base.
%### 
\subsubsection{Division by Zero Fitness Function}
%###
Like the \texttt{Memory Access} fitness function, the \texttt{Division by Zero} fitness function is also
designed to find vulnerabilities in the program under test instead of simply improving test input quality. In
particular, this fitness function aims to uncover a single class of denial of service vulnerabilities -
unhandled arithmetic exceptions that arise from performing division by zero.

When the program under test processes an input file, it usually executes multiple distinct division
instructions, and many of them - multiple times, so there is no simple and obvious way to directly assign a
fitness value to an input file. The first idea is to record all observed instances of division and choose the
smallest absolute value of a divisor as the fitness score. However, such an assignment is not very beneficial
to the diversity of the inputs' gene pool as it makes it converge to smaller files with a minimum number of
executed divisions as long as there is at least one divisor close to zero. Such a constriction opposes the
goal of triggering a division exception as it artificially and unnecessarily limits the number of sites
that are considered for exploration. 

To counteract this effect additional secondary objectives were put in place. The first secondary objective is
the \texttt{MaximizeDivisionsSecondaryObjective} and its goal is to enlarge the number of sites in the
program under test that are being explored by the search process by increasing the number of division
instructions covered by a suite. Using only this single secondary objective, however, leads to individuals
simply growing in size over the course of evolution, while keeping at least one close-to-zero divisor. While
this is beneficial to the diversity of the suites, it causes an unwelcome strain on the performance, which
is why another secondary - or rather a tertiary - objective has been added - the
\texttt{MinimizeCumulativeZeroDistanceSecondaryObjective}. The goal of this objective is to prevent an
uncontrolled growth and ensure overall improvement of quality of the individuals by keeping the cumulative
divisor distance to zero as small as possible.

At runtime the pintool component of the \texttt{Division by Zero} fitness function intercepts all invocations
of the \texttt{DIV} and \texttt{IDIV} x86 instructions and records the divisor value in a list, which is
associated with the address of the instruction in question by means of a map container. When the execution of
the system under test has ended, for each division instruction the pintool keeps only the divisor with the
smallest zero distance when it packages the message to be sent out to its \java fitness function counterpart.
There is a reason behind selecting the best divisor only after the main runtime instead of doing it on-the-fly,
even though it wastes some memory space for storing all observed divisors. This is an instance of the classical
runtime vs memory requirement trade off: by having injected code simply append any new value to a list
instead of doing comparisons to decide whether to keep or reject the new value, the code is simple enough to
be inlined and thus performs considerably faster; and with main memory being available in abundance on  modern
computers and execution speed being the most limiting factor, it seems like a good trade off to make.

Because the \texttt{Division by Zero} fitness function is meant to be used for vulnerability scanning, it is
exclusively compatible with the singleton population evolution mode introduced in \cref{sec:memcov}. The \java
component of this fitness function keeps a record of all executed division instructions alongside the absolute
value of their divisor closest to zero for each file in a suite. The overall fitness score of the entire suite
is then determined by finding the minimal divisor distance over all division instructions over all files in
the suite.

As previously suggested, this fitness function currently only supports integer based division instructions
because \pin imposes a limitation on retrieving floating point values from
registers\footnote{\url{https://software.intel.com/sites/landingpage/pintool/docs/67254/Pin/html/index.html\#FP}}.
Working around this and adding support for \texttt{SIMD} instructions are some planned future improvements.

An intuition is that it is unusual for valid well formed inputs to cause divisions by zero in the program under
test. It is therefore recommended to use the local search feature described in \cref{sec:local}, which is
capable of producing values beyond the range of validity, in order to increase the chances of actually
triggering arithmetic exceptions.

%### 
%\subsubsection{Context Switches Fitness Function}
%### 
% TODO mention in future work section
 
%### 
\subsubsection{Integer Overflow Fitness Function}
%### 
Continuing in the vein of fitness functions specialized in narrow categories of vulnerabilities, the next
fitness function is the \texttt{Integer Overflow Fitness Function}. Just as the name suggests, its aim is to
guide the evolution of inputs towards causing an unhandled integer overflow. Integer overflows are often
suitable as vessels for denial of service attacks as well as access control attacks, and therefore of
critical severity. 

In order to achieve its goal the \texttt{Integer Overflow} fitness function\ldots
\change[inline]{complete}
% method - multiplications (and additions)
% need for local search
% singleton population
% standard secondary objectives?

%### 
\subsubsection{Buffer Overflow Fitness Function}
%###
\change[inline]{complete}
% motivation
% method and relation to memory access fitness function
% mention implementation difficulties:
% 	multithreading issues -> interleaving -> solution additionally map by thread_id / use thread local storage
% 	however point_after is unreliable (https://software.intel.com/sites/landingpage/pintool/docs/67254/Pin/html/#Invocation) and so a replacement must be used
% singleton population
% secondary objectives
% local search