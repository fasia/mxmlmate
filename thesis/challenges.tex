%###
\subsection{Challenges}
%###
Getting away from pure \java and adding support for application binaries as test subjects 
to \xmlmate by transforming it from 
a single \java application into a set of loosely coupled, yet interdependent and distinct 
processes written in different programming languages presented a number of challenges from 
such fields like software engineering and system architecture design. 
In the following sections I would like to shed some light onto 
the most noteworthy of them.
%###
\subsubsection{Chromosome Representation}
%###
\label{sec:repr}
\xmlmate is implemented on top of the genetic algorithm framework that is part of 
\evosuite{}\cite{fraser2013whole} and as such must adhere to its specifications and structures 
of genetic representations. 
Because \evosuite as a whole is aimed at generating {\small JUnit} tests for \java applications, 
the entities that comprise the population  (i.e. the \emph{chromosomes} or \emph{individuals}) 
are called \emph{test suites}. The test suites are what evolves from generation to generation.
In turn, the test suites consist of \emph{tests}. Similarly, \xmlmate has adopted the concepts of 
test suite and test - they correspond to a set of \xml trees and a single \xml tree, respectively. 
They are also sometimes referred to as \emph{test suite chromosomes} and \emph{test chromosomes}, resp.

To ease understanding of the representation of \xml test chromosomes, \cref{lst:repr} offers an attempt at
visualization on an excerpt from the \texttt{pcap} file format example further described in
\cref{sec:formats:pcap}. In \xmlmate all \xml trees are governed by a single \xsd (depicted in the top frame)
that is given as a parameter at program start. In order to implement the genetic operation \emph{mutation} for
test chromosomes more or less efficiently, each node in a test's \xml tree should have a reference to its
corresponding definition in the schema (represented as blue dotted lines). This is accomplished by extending
some of the \java classes provided by \xom to include the references in question as well as methods that maintain
their relevance and correctness. This alone, however, is not sufficient for implementing an efficient
\emph{crossover} of two \xml trees.

To quickly find sites suitable for crossover in two test chromosomes, an additional mapping is needed: 
one that associates definitions in the global schema with nodes that are actually present in the \xml tree, so 
as to be able to find compatible intersections.
Since I am using \xerces for storing and accessing the schema information, and it is only exposed by means of 
\java interfaces, it was not possible to add this mapping to local copies of the schema  
enhanced with the needed references to test chromosomes. However, what could be added were local 
maps referencing the global definition instances as keys and local nodes as values. One such mapping is depicted
in \cref{lst:repr} as the bottom frame, and its values are orange dashed lines. A disadvantage is that
additional methods had to be added to ensure the relevance of these mappings, as the node composition of an
\xml tree is constantly changing. One benefit of this approach is the reduced memory consumption as the schema
definition must only be stored once globally, and the local mappings are nothing but sets of references 
internally and thus very lightweight. 

The overall \xml test chromosome consists of an \xml tree instance together with its accompanying element map,
denoted by a green dashdotted frame in \cref{lst:repr}.

\begin{listing}[H]
\centering
\begin{tikzpicture}
\tikzset{execute at begin node=\strut}
\begin{scope}[level distance=90pt,sibling distance=0pt,grow=right]
\Tree [.\node(schema){schema};
[.\node(posint){positiveInt}; ]
[.\node(pcap){pcap}; ]
[.\node(header){GlobalHeader};
	[.\node(network){network}; positiveInt ]
	[.\node(snaplen){snaplen}; positiveInt ]
	[.\node(sigfigs){sigfigs}; \node(unsint){unsignedInt}; ]
	[.\node(thiszone){thiszone}; int ]
	[.\node(vmi){version\_minor}; \node(vmival){fixed(4)}; ]
	[.\node(vma){version\_major}; \node(vmaval){fixed(2)}; ]
]
[.\node(pheader){PacketHeader}; \node(packetval){$\ldots$}; ]
]
\node[draw,fit=(schema)(posint)(packetval)(unsint)]{};
\node[above =3cm of schema]{XML Schema};
\end{scope}
%
\begin{scope}[shift={(0in,-0.2cm)}]
\begin{scope}[shift={(0in,-2.5in)},frontier/.style={distance from root=290pt},level distance=70pt,sibling
distance=0pt,grow'=right]
\Tree [.\node(inspcap){pcap};
[.\node(inspacket){packet}; \node(inspacketval){$\ldots$}; ]
[.\node(insheader){header};
[.\node(insnetwork){network}; \node(insnetworkval){1}; ]
[.\node(inssnaplen){snaplen}; \node(inssnaplenval){80}; ]
[.\node(inssigfigs){sigfigs}; 40 ]
[.\node(insthiszone){thiszone}; 0 ]
[.\node(insvmi){version\_minor}; \node(insvmival){4}; ]
[.\node(insvma){version\_major}; \node(insvmaval){2}; ] ] ]
\node[draw,fit=(inspcap)(insvmaval)(inspacketval)]{};
\node[above =1.5cm of inspcap](dummy1){XML Instance};
\end{scope}
%
\begin{scope}[node distance=1.6cm, auto, shift={(0in,-4.5in)}]
\node[draw](decl1){pcap};
\node[draw,right = of decl1](decl2){GlobalHeader};
\node[right = of decl2](decl3){$\ldots$};
\node[draw,right = of decl3](decl4){positiveInt};
\node[draw,fit=(decl1)(decl4)](elemap){};
\node[above =0.3cm of decl1](dummy2){Element Map};
\node[below right=0.01pt of decl4](dummyend){};
\end{scope}
\end{scope}
\node[draw,thick,loosely dashdotted,ForestGreen,fit=(dummy1)(dummyend)](instance){};
\draw[blue,dotted,thick,->] (inspcap)..controls +(north:2) and +(west:4.7)..(pcap);
\draw[blue,dotted,thick,->] (inspacket)..controls +(north west:4) and +(south west:4)..(pheader);
\draw[blue,dotted,thick,->] (insheader)..controls +(north west:2) and +(south west:3)..(header);
\draw[blue,dotted,thick,->] (insnetworkval)..controls +(north west:4) and +(east:4)..(posint);
\draw[blue,dotted,thick,->] (inssnaplenval)..controls +(north west:4) and +(east:4)..(posint);
\draw[blue,dotted,thick,->] (insvmival)..controls +(north east:1) and +(south east:4)..(vmival);
\draw[blue,dotted,thick,->] (insvmaval)..controls +(north east:1.5) and +(south east:4)..(vmaval);
\draw[blue,dotted,thick,->] (insnetwork)..controls +(north:3) and +(south:3)..(network);
\draw[blue,dotted,thick,->] (inssnaplen)..controls +(west:1.5) and +(west:1.5)..(snaplen);
\draw[blue,dotted,thick,->] (inssigfigs)..controls +(west:2) and +(west:2)..(sigfigs);
\draw[blue,dotted,thick,->] (insthiszone)..controls +(east:2.5) and +(east:2)..(thiszone);
\draw[blue,dotted,thick,->] (insvmi)..controls +(east:3) and +(east:2)..(vmi);
\draw[blue,dotted,thick,->] (insvma)..controls +(east:3.5) and +(east:2.5)..(vma);

\draw[orange,dashed,thick,->] (decl1)..controls +(north:3.5) and +(south:2.5)..(inspcap);
\draw[orange,dashed,thick,->] (decl2)..controls +(north:3.5) and +(south:2.5)..(insheader);
\draw[orange,dashed,thick,->] (decl3)..controls +(north:1) and +(east:5)..(insnetwork);
\draw[orange,dashed,thick,->] (decl3)..controls +(north:1) and +(east:4.5)..(inssnaplen);
\draw[orange,dashed,thick,->] (decl3)..controls +(north:1) and +(east:4)..(inssigfigs);
\draw[orange,dashed,thick,->] (decl3)..controls +(north:1) and +(east:3.5)..(insthiszone);
\draw[orange,dashed,thick,->] (decl3)..controls +(north:1) and +(east:3)..(insvmi);
\draw[orange,dashed,thick,->] (decl3)..controls +(north:1) and +(east:2.5)..(insvma);
\draw[orange,dashed,thick,->] (decl3)..controls +(north:1.5) and +(west:2)..(insvmival);
\draw[orange,dashed,thick,->] (decl3)..controls +(north:2) and +(west:1.5)..(insvmaval);
\draw[orange,dashed,thick,->] (decl4)..controls +(north:3.5) and +(south west:1.5)..(insnetworkval);
\draw[orange,dashed,thick,->] (decl4)..controls +(north:2) and +(south west:1)..(inssnaplenval);
\end{tikzpicture}
\caption{Chromosome Representation}
\label{lst:repr}
\end{listing}

%### 
\subsubsection{Communication Protocol}
%###
\label{sec:proto}
Since \xmlmate now comprises several processes, a communication protocol must be put in place to 
interconnect the individual components. The communication channels themselves are built on top of
the \zmq messaging library with \msgpack as the serialization/deserialization engine. The current
implementation uses tcp\footnote{TCP loopback is used because the pure \java implementation 
of \zmq does not support \texttt{ipc} on the Windows platform, which I do most of the development on.
Using a JNI based Java wrapper as a drop-in replacement would alleviate this problem, 
but it is not considered to be worth the effort.} 
as a carrier for the messages; however, it is entirely possible
to change this to inter-process communication just by changing a configuration option, should it 
become necessary. As yet, it is very far from being a bottleneck.


After each evolution step a test suite usually contains several tests that changed due to mutation and
crossover. When the fitness of the suite is requested to be computed next time by the genetic algorithm, 
only the tests that changed should be evaluated. For each of those a file is written and a packet is 
formed consisting of a number identifying the test inside the test suite and the path to its corresponding 
file. This packet is serialized into a byte string with \msgpack and then sent as a \zmq message via a 
\texttt{PUSH} socket on a well known port (e.g. the default is 5556). After all messages have been sent,
a response is awaited for each of those sent out tasks. Responses are collected as \zmq messages coming 
in via a \texttt{PULL} socket on another well known port (default 5557) and deserialized with \msgpack 
into the identifying number and whatever data format is expected by the current fitness function. 
(E.g. a fitness function that counts the number of executed basic blocks expects an array of addresses.)
As far as the \java part of \xmlmate is concerned this is all it ever sees of the entire process, and so it 
must rely on the pintool monitoring the application under test to collect and send back fitness results.

A load balancer has three \zmq sockets through which it communicates with the rest of the system - they are
called \emph{frontend}, \emph{backend} and \emph{forward}. When a message is received on the frontend,
which is a \texttt{PULL} type socket, it is sent to the balancer's next available worker through
the backend \texttt{ROUTER} type socket, and the worker is removed from the available worker queue. When the
response arrives from a worker on the backend, it is redirected through the forward \texttt{PUSH} type socket
further down the processing pipeline, and the worker is returned to the available worker queue. 

Whenever a new worker wishes to join the balancer it sends a \texttt{RDY} message to the balancer's backend,
which causes the worker's identity to be added to the queue of available workers. Similarly, when a worker
process dies, its lifeguard signals the balancer with a \texttt{DEAD} message, which makes it forward a
message with the corresponding input file number and the \texttt{dead} flag set to true further down the
processing pipeline; the worker's identity is also removed form the worker queue. The most common system setup
consists of two balancers operating on ports 5556, 5570, 5560, and 5560, 5580, 5557 as their frontends,
backends, and forwards, respectively.

A format converter has a single \texttt{DEALER} type socket operating on default port 5570.
When a format converter is first started, it registers itself with its load balancer facade by sending a
\texttt{RDY} message. Afterwards it receives \zmq messages, unpacks them with \msgpack, stores their
identifying number, converts the file, packs the stored number followed by the path to the converted file with
\msgpack into new \zmq messages and sends them out to its load balancer as a response.

Similarly to format converters, a test driver also has a \texttt{DEALER} type socket, which operates on the
default port 5580 and with which it registers itself with its load balancer by sending \texttt{RDY}. After
registration, the test driver listens for incoming \zmq messages and unpacks them with \msgpack
into the identifying number and a path. It then calls \texttt{PIN\_SCORE\_START()} to signal the start of the
execution of the system under test to the monitoring pintool, whereafter it passes the received file path to
the sut for execution. When the sut finishes and control returns to the test driver, it calls
\texttt{PIN\_SCORE\_END(number)} with the number stored from the message earlier.

When a test driver invokes \texttt{PIN\_SCORE\_START()}, this invocation gets replaced with a call to a method
in the pintool that clears its internal information buffers, thereby resetting all currently gathered fitness
related information. When the test driver then calls \texttt{PIN\_SCORE\_END(number)}, this call gets replaced
with one that packs and sends out a message containing the identifying number and any fitness information via
the same \zmq \texttt{DEALER} socket the test driver received the original message on. This is possible
because the pintool and the test driver run in the same process, which means they both have access to the same
memory, and so a pointer can be easily shared between the two components.

Because test drivers can sometimes terminate unpredictably and unexpectedly (e.g. due to receiving an input
file that causes them to fail because of a vulnerability), they are always started and guarded by special
lifeguard processes. Whenever control returns to the lifeguard process itself, which means that the guarded
process has terminated, it assumes its identity, creates a \texttt{DEALER} socket and sends out a \texttt{DEAD}
message to the load balancer. Afterwards, it destroys the socket and restarts the test driver.

Please note that all ports mentioned above are customizable and even the individual message carriers for
each leg of the communication pathway can be changed from tcp to any other protocol supported by \zmq, of which
there are plenty.

The described system contains quite a number of components and it is at least somewhat important to start them
up in a manner that does not exhibit the slow joiner syndrome - a condition, when a worker which is slow to
join the system, does not receive its fair share of work items because all of them were already distributed
among its faster peers. To avoid this pitfall, it is advisable to start up the load balancers first, followed
by converter and test driver processes, and then finally \xmlmate itself. To make sure that all other
processes have formed the necessary connections, the default behavior of \xmlmate is to wait for a maximum of
thirty seconds for a user input confirming the readiness of all other system components. After this timeout the
process will be started automatically, thus making it well suited for unsupervised use.

\change[inline]{make pretty communication graph}
%###
\subsubsection{Format Converters}
%###
One of the main tasks of this work is to show that \xmlmate is versatile enough to not be limited to 
only \xml and \xml-derived formats. In order to facilitate this versatility the concept of a format 
converter has been introduced. The main goal of a converter for a specific format is to convert an 
\xml file corresponding to a schema specification into another format as specified by the purpose 
of this converter. For an example consider a converter from \xml to the packet capture format \pcap. 
This converter expects \xml files in the format corresponding to the \pcap{} \xsd as shown in
\cref{lst:xsdexample} and produces a valid \pcap file. \info{make a section reference}


\xmlmate also has support for starting out its evolution process from a given set of inputs. If the   
inputs are not available in the \xml format, they must first be converted so as to be genetically 
representable. For this use case there are reverse converters, which convert files into their 
\xml equivalent, again, according to the corresponding \xsd. For the above example a reverse converter 
could transform any valid \pcap file into its \xml representation governed by the \pcap{} \xsd.
%###
\subsubsection{Parallelization}
%###
\label{sec:par}
After running the first tests on the initial \xmlmate design, which did not include any parallelization, 
it quickly became clear that both format conversion and running the program under test with \pin 
are two performance bottlenecks significantly slowing down the entire process. Considering that the 
tests were run on a multi-core machine, and only one CPU was ever doing any work, the decision was 
made to make both the conversion and evaluation steps parallel by allowing arbitrarily many 
converter and test driver instances (along with their corresponding pintools) to participate in the process. 
This was somewhat easily accomplished due to the very well thought out design of the messaging library \zmq, 
as there were only minor issues to consider - like adding the static broker component to 
manage the dynamic number of converters and test drivers.

However, after adding the aforementioned parallelization, a new bottleneck presented itself - this time 
somewhere inside the \java part of \xmlmate. Using a \java profiling tool I was able to trace the cause 
to the mutation and crossover routines, which seems logical in retrospect because both manipulate the 
in-memory representations of \xml tree structures. That alone, however, is not sufficient to cause a 
major bottleneck. \evosuite's genetic algorithm always makes backups of entire test suites as well as 
single tests before applying any genetic operation to them in order to be able to recover from an 
unfavorable outcome (e.g. decreased fitness) and roll back all changes. So each time an in-memory \xml 
tree was going to be mutated, it was deeply copied, each time two \xml trees were going to be crossed 
over, they were both deeply copied, each time an entire test suite was going to be mutated, it was 
deeply copied - each of its tests one after another, and each time two test suites were about to 
undergo crossover, both of them were deeply copied. That was what caused this particular bottleneck. 
To remedy this situation I applied the following two improvements:

The first one was adding three distinct task executors dividing the responsibilities of deeply copying, mutating and 
crossing over \xml trees. This way all three classes of tasks were enabled to run simultaneously 
no longer needing to queue up linearly one strictly after another. The mutation task executor is 
responsible for taking a single individual \xml tree, copying it and mutating the copy; the 
crossover task executor takes two \xml trees, delegates their copying to the copying task executor 
for a speed up, and then performs the actual crossover on the returned copies.

The second improvement was making the process of copying the test suites shallow instead of deep. 
This is a safe change since the individual tests are still copied before any changes are applied 
to them - the result is a kind of \emph{copy-on-maybe-write} policy as only those tests are copied 
that are going to experience potential change.

After that there were still other bottlenecks - some more surprising than others. For example 
it was rather unexpected to see calls to methods of the \texttt{Random} class being listed among the most time
consuming by the profiler. In \java 7 the \texttt{Random} class is thread-safe, which has lead to a lot of 
contention when multiple threads suddenly started asking the singleton instance of \evosuite's randomness 
provider for random values for mutation and crossover. The fix was rather easy: \java 7 also provides 
a \texttt{ThreadLocalRandom} class intended for exactly this kind of situation; this easy fix came 
at a price, however - it is no longer possible to provide a single seed value to the whole generation 
process.

Another minor performance optimization included adding a polymorphic method for selecting a random 
element from a given \texttt{Set}. The previously available methods treat a \texttt{Set} like a 
generic \texttt{Collection} and perform the selection by first copying the entire set into a 
\texttt{RandomAccess}-able \texttt{List}, choosing an element at a random index, and then discarding the list.
This is clearly wasteful of resources in case the set in question is large. In this case it is actually faster
to generate a random number between zero and the number of elements in the set and then actuate the set's
iterator that number of times finally claiming the element acquired last as the required random choice.

One further possible major optimization could be to allow partial fitness score evaluation while a suite 
mutation is still in progress. When an \xml file has left the mutation or crossover executor it could be 
immediately sent out to a worker process for fitness evaluation. Preliminary tests showed promising 
results; however, this optimization is very intrusive of \evosuite's concept of a single threaded 
genetic algorithm possibly leading to grave incompatibilities with other mechanisms deep within other parts 
of \evosuite, therefore it was deemed as a neat future work item, which should probably be combined with a 
proper restructuring of the genetic framework.

Another optimization, which sadly also belongs to the list of currently incompatible changes, would require to
rewrite the genetic algorithm in order to enable it to process the individual test suites in parallel -
not only mutation and crossover, but evaluation as well, leaving generation alternations to be the only
thresholds to otherwise completely parallel execution.
%###
\subsubsection{Local Search}
%###
\label{sec:local}
\info{update for range inversion after it's implemented}
As previously mentioned, one of the core principles of \xmlmate was to always create schema-valid inputs; 
however, this no longer complies with current requirements in the use case of security testing as the 
inputs that most often cause the targeted program to fail usually deviate from the specification in 
one way or another. The case where the syntactic validity of the \xml representation is violated is 
both unfavorable to format conversion and out of scope of this work.

Fortunately, it is possible to use another artifact of \evosuite's design to provide deviations from the 
specification in values (as opposed to structure): the local search. 
The local search is a mechanism which allows to perform small mutations in limited contexts in between 
the regular genetic operations to find individuals that are ``closely related'' to existing ones
yet have better fitness. One implementation of this is interval bounded numeric offset mutation whereby
several numeric values in the individual are changed by an offset chosen at random from a predefined 
interval. E.g. integer values are changed by random values from $[-10,10]$ and floating point numbers 
are offset by values from $[-0.5,0.5]$.
Performing such changes sometimes leads to the values leaving their value spaces specified in the schema, 
which allows to accommodate for the new use case of security testing.

Quite ironically, \evosuite's genetic algorithm framework for some reason does not create a backup 
copy of individuals before initiating local search, which manifested itself in abruptly diminishing 
fitness values; this was, of course, fixed by issuing a backup copy inside the local search procedure, 
additionally taking full advantage of the newly implemented parallelized deep copying mechanism.

Please note that due to its specification ignoring nature, the local search is currently incompatible with the
schema coverage fitness function described in \cref{sec:fit:schema} as the algorithm responsible for computing
how many of the schema's definitions are used in an \xml instance gets confused by unexpected invalid values.
This seems to be a rather small issue and will probably be fixed at a later point in time; however, as of now
there has been no reason to run schema coverage based evolutions with local search enabled, which, at least for
the time being, makes this a non-issue.
